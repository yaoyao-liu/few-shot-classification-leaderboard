---
title: miniImageNet
layout: leaderboard
---

## *mini*ImageNet Leaderboard (5-class)

#### [Edit this leaderboard](https://github.com/yaoyao-liu/few-shot-classification-leaderboard/edit/main/miniimagenet.md)

Method   | Venue | Year | Backbone   | Setting | 1-shot      | 5-shot   | Code | Reported by 
------- | ------ | ---- | --------   | -----    | -----   | -----    | ---- | ----
[HCTransformers](https://arxiv.org/pdf/2203.09064v1.pdf) | CVPR | 2022 | ViT-S | Inductive | 74.74 ± 0.17 | 89.19 ± 0.13 | [\[PyTorch\]](https://github.com/StomachCold/HCTransformers) | [\[Source\]](https://arxiv.org/pdf/2203.09064v1.pdf)
[Meta DeepBDC](https://arxiv.org/pdf/2204.04567.pdf) | CVPR | 2022 | ResNet-12 | Inductive | 67.34 ± 0.43 | 84.46 ± 0.28 | [\[PyTorch\]](https://github.com/Fei-Long121/DeepBDC) | [\[Source\]](https://arxiv.org/pdf/2204.04567.pdf)
[STL DeepBDC](https://arxiv.org/pdf/2204.04567.pdf) | CVPR | 2022 | ResNet-12 | Inductive | 67.83 ± 0.43 | 85.45 ± 0.29 | [\[PyTorch\]](https://github.com/Fei-Long121/DeepBDC) | [\[Source\]](https://arxiv.org/pdf/2204.04567.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 3xResNet012 | Transductive | 84.04 ± 0.23 | 89.14 ± 0.11 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 2xResNet-12(1/√2) | Transductive | 82.31 ± 0.24 | 88.57 ± 0.12 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 3xResNet-12 | Inductive | 71.75 ± 0.19 | 87.15 ± 0.12 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 2xResNet-12(1/√2) | Inductive | 70.63 ± 0.20 | 86.28 ± 0.12 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[GNN](https://arxiv.org/pdf/1711.04043.pdf) + [FT](https://arxiv.org/pdf/2001.08735.pdf) | ICLR | 2020 | ResNet-10 | Inductive | 66.32 ± 0.80 | 81.98 ± 0.55 | [\[PyTorch\]](https://github.com/hytseng0509/CrossDomainFewShot) | [\[Source\]](https://arxiv.org/pdf/2001.08735.pdf)
[MetaOptNet](https://arxiv.org/pdf/1904.03758.pdf)     | CVPR   | 2019 | ResNet-12  | Inductive | 62.64 ± 0.61    | 78.63 ± 0.46     | [\[PyTorch\]](https://github.com/kjunelee/MetaOptNet) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[MTL](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf)     | CVPR   | 2019 | ResNet-12  | Inductive | 61.2 ± 1.8    | 75.5 ± 0.8    | [\[PyTorch\]](https://github.com/yaoyao-liu/meta-transfer-learning/) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf)
[TADAM](http://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning.pdf)     | NeurIPS   | 2018 | ResNet-12  | Inductive | 58.5 ± 0.3    | 76.7 ± 0.3     | [\[TensorFlow\]](https://github.com/ElementAI/TADAM) | [\[Source\]](http://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning.pdf)
[MAML](https://arxiv.org/pdf/1703.03400.pdf) | ICML | 2017 | 4CONV | Inductive | 48.70 ± 1.75 | 63.11 ± 0.92  | [\[TensorFlow\]](https://github.com/cbfinn/maml) | [\[Source\]](https://arxiv.org/pdf/1703.03400.pdf)
[LEO](https://arxiv.org/pdf/1807.05960.pdf) | ICLR | 2019 | WRN-28-10 | Inductive | 61.76 ± 0.08 | 77.59 ± 0.12 | [\[TensorFlow\]](https://github.com/deepmind/leo) | [\[Source\]](https://arxiv.org/pdf/1807.05960.pdf)
[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | 4CONV | Inductive | 49.42 ± 0.78 | 68.20 ± 0.66  | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://arxiv.org/pdf/1703.05175.pdf)
[MatchingNets](https://arxiv.org/pdf/1606.04080.pdf) | NeurIPS | 2016 | 4CONV | Inductive | 43.56 ± 0.84 | 55.31 ± 0.73  | [\[TensorFlow\]](https://github.com/AntreasAntoniou/MatchingNetworks) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[RelationNets](https://arxiv.org/pdf/1711.06025.pdf) | CVPR | 2018 | 4CONV | Inductive | 50.44 ± 0.82 | 65.32 ± 0.70  | [\[PyTorch\]](https://github.com/floodsung/LearningToCompare_FSL) | [\[Source\]](https://arxiv.org/pdf/1711.06025.pdf)
[TPN](https://arxiv.org/pdf/1805.10002.pdf) | ICLR | 2019 | 4CONV | Transductive | 55.51 ± 0.86 |  69.86 ± 0.65  | [\[TensorFlow\]](https://github.com/csyanbin/TPN) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[AdaResNet](https://arxiv.org/pdf/1712.09926.pdf) | ICML | 2018 | ResNet-12 | Inductive |  56.88 ± 0.62 | 71.94 ± 0.57  | None | [\[Source\]](https://arxiv.org/pdf/1711.06025.pdf)
[DeepEMD](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Inductive |  65.91 ± 0.82 | 82.41 ± 0.56  | [\[PyTorch\]](https://github.com/icoz69/DeepEMD) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | ResNet-12 | Inductive | 60.37 ± 0.83 | 78.02 ± 0.57  | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[MatchingNets](https://arxiv.org/pdf/1606.04080.pdf) | NeurIPS | 2016 | ResNet-12 | Inductive | 63.08 ± 0.80 | 75.99 ± 0.60  | [\[TensorFlow\]](https://github.com/AntreasAntoniou/MatchingNetworks) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[CTM](https://arxiv.org/pdf/1905.11116.pdf) | CVPR | 2019 | ResNet-18 | Inductive | 64.12 ± 0.82  | 80.51 ± 0.13  | [\[PyTorch\]](https://github.com/Clarifai/few-shot-ctm) | [\[Source\]](https://arxiv.org/pdf/1905.11116.pdf)
[wDAE-GNN](https://arxiv.org/pdf/1905.01102.pdf) | CVPR | 2019 | WRN-28-10 | Inductive | 61.07 ± 0.15 | 76.75 ± 0.11  | [\[PyTorch\]](https://github.com/gidariss/wDAE_GNN_FewShot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[PPA](https://arxiv.org/pdf/1706.03466.pdf) | CVPR | 2018 | WRN-28-10 | Inductive | 59.60 ± 0.41 | 73.74 ± 0.19  | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[CAN](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Inductive |  63.85 ± 0.48 | 79.44 ± 0.34 | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
[CAN+T](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Transductive |  67.19 ± 0.55 | 80.64 ± 0.35  | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | ResNet-12 | Inductive |  66.78 ± 0.20 | 82.05 ± 0.14  | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | 4CONV | Inductive |  55.15 ± 0.20 | 71.61 ± 0.16  | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | WRN-28-10 | Inductive |  65.10 ± 0.20 | 81.11 ± 0.14 | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
[Dhillon et al.](https://openreview.net/pdf?id=rylXBkrYDS) | ICLR | 2020 | WRN-28-10 | Transductive |  65.73 ± 0.68 | 78.40 ± 0.52 | None | [\[Source\]](https://openreview.net/pdf?id=rylXBkrYDS)
[SIB](https://openreview.net/pdf?id=Hkg-xgrYvH) | ICLR | 2020 | WRN-28-10 | Transductive |  70.0 ± 0.6 | 79.2 ± 0.4 | [\[PyTorch\]](https://github.com/hushell/sib_meta_learn) | [\[Source\]](https://openreview.net/pdf?id=Hkg-xgrYvH)
[SIB](https://openreview.net/pdf?id=Hkg-xgrYvH) | ICLR | 2020 | 4CONV | Transductive |  63.26 ± 1.07 | 75.73 ± 0.71 | [\[PyTorch\]](https://github.com/hushell/sib_meta_learn) | [\[Source\]](https://openreview.net/pdf?id=Hkg-xgrYvH)
[KTN](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf) | ICCV | 2019 | 4CONV | Inductive |  64.42 ± 0.72 | 74.16 ± 0.56 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf)
[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | 4CONV | Inductive | 49.07 ± 0.43 | 65.73 ± 0.36 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Inductive | 60.71 | 77.64 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
[Variational method](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Inductive |  61.23 ± 0.26 | 77.69 ± 0.17 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf)
[Variational method](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | 4CONV| Inductive |  57.15 ± 0.31  | 71.54 ± 0.23 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf)
[TEAM](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | 4CONV | Transductive |  56.57  |  72.04 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf)
[TEAM](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Transductive |  60.07  | 75.90 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf)
[Robust 20-dist++](https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Inductive |63.73 ± 0.62  | 81.19 ± 0.43| [\[PyTorch\]](https://github.com/dvornikita/fewshot_ensemble) | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf)
[DeepEMD v2](https://arxiv.org/pdf/2003.06777.pdf) | arXiv | 2020 | ResNet-12 | Inductive |  68.77 ± 0.29 | 84.13 ± 0.53  | [\[PyTorch\]](https://github.com/icoz69/DeepEMD) | [\[Source\]](https://arxiv.org/pdf/2003.06777.pdf)
[E<sup>3</sup>BM](https://arxiv.org/pdf/1904.08479.pdf) | ECCV | 2020 | ResNet-12 | Inductive |  63.8 ± 0.4 | 80.1 ± 0.3  | [\[PyTorch\]](https://gitlab.mpi-klsb.mpg.de/yaoyaoliu/e3bm) | [\[Source\]](https://arxiv.org/pdf/1904.08479.pdf)
[E<sup>3</sup>BM](https://arxiv.org/pdf/1904.08479.pdf) | ECCV | 2020 | WRN-28-10 | Transductive |  71.4 ± 0.5 | 81.2 ± 0.4  | [\[PyTorch\]](https://gitlab.mpi-klsb.mpg.de/yaoyaoliu/e3bm) | [\[Source\]](https://arxiv.org/pdf/1904.08479.pdf)
[LST](https://papers.nips.cc/paper/9216-learning-to-self-train-for-semi-supervised-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Semi-supervised |  70.1 ± 1.9 | 78.7 ± 0.8  | [\[TensorFlow\]](https://github.com/xinzheli1217/learning-to-self-train) | [\[Source\]](https://papers.nips.cc/paper/9216-learning-to-self-train-for-semi-supervised-few-shot-classification.pdf)
[AM3](https://papers.nips.cc/paper/8731-adaptive-cross-modal-few-shot-learning.pdf) | NeurIPS | 2019 | ResNet-12 | Inductive |  65.30 ± 0.49 | 78.10 ± 0.36  | [\[TensorFlow\]](https://github.com/ElementAI/am3) | [\[Source\]](https://papers.nips.cc/paper/8731-adaptive-cross-modal-few-shot-learning.pdf)
[DSN-MR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Transductive |  64.60 ± 0.72 | 79.51 ± 0.50  | [\[PyTorch\]](https://github.com/chrysts/dsn_fewshot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Transductive |  67.77 ± 0.32 | 84.60 ± 0.43  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | 4CONV | Transductive |  66.01 ± 0.36 | 82.83 ± 0.41  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[AFHN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  ResNet-18 | Inductive |  62.38 ± 0.72 | 78.16 ± 0.56 | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  ResNet-12 | Transductive |  66.80 | 79.26 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  ResNet-12 | Semi-supervised |  69.66 | 80.11 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[ICI v2](https://arxiv.org/pdf/2007.08461.pdf) | TPMAI | 2021 |  ResNet-12 | Semi-supervised |  73.12 ± 0.65 | 83.28 ± 0.37 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://arxiv.org/pdf/2007.08461.pdf)
[ICI v2](https://arxiv.org/pdf/2007.08461.pdf) | TPMAI | 2021 |  ResNet-12 | Transductive |  72.39 ± 0.62 | 83.27 ± 0.33  | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://arxiv.org/pdf/2007.08461.pdf)
[TransMatch](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_TransMatch_A_Transfer-Learning_Scheme_for_Semi-Supervised_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  WRN-28-10 | Semi-supervised |  63.02 ± 1.07 | 82.24 ± 0.59 | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_TransMatch_A_Transfer-Learning_Scheme_for_Semi-Supervised_Few-Shot_Learning_CVPR_2020_paper.pdf)
[∆-encoder](https://arxiv.org/pdf/1806.04734.pdf) | NeurIPS | 2018 |  ResNet-18 | Inductive |  59.9 | 69.7 | [\[TensorFlow\]](https://github.com/EliSchwartz/DeltaEncoder) | [\[Source\]](https://arxiv.org/pdf/1806.04734.pdf)
[Neg-Cosine](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490426.pdf) | ECCV | 2020 |  ResNet-12 | Inductive |  63.85 ± 0.81 | 81.57 ± 0.56 | [\[PyTorch\]](https://github.com/bl0/negative-margin.few-shot) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490426.pdf)
[J. Kim et al.](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460579.pdf) | ECCV | 2020 |  ResNet-12 | Inductive |  65.08 ± 0.86 |  82.70 ± 0.54 | [\[PyTorch\]](https://github.com/jaekyeom/MABAS) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460579.pdf)
[BD-CSPN](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460715.pdf) | ECCV | 2020 |  WRN-28-10 | Transductive |  70.31 ± 0.93 |  81.89 ± 0.60 | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460715.pdf)
[Centroid alignment](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500018.pdf) | ECCV | 2020 |  WRN-28-10 | Inductive |  65.92 ± 0.60 | 82.85 ± 0.55 | [\[PyTorch\]](https://github.com/ArmanAfrasiyabi/associative-alignment-fs) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500018.pdf)
[ICA + MSP](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf) | ECCV | 2020 | DenseNet | Transductive |  77.06 ± 0.26 | 84.99 ± 0.14 | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf)
[ICA + MSP](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf) | ECCV | 2020 | DenseNet | Semi-supervised | 80.11 ± 0.25 |  85.78 ± 0.13 | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf)
[Y. Tian et al.](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf) | ECCV | 2020 | ResNet-12 | Inductive | 64.82 ± 0.60 | 82.14 ± 0.43 | [\[PyTorch\]](https://github.com/WangYueFt/rfs) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf)
[EPNet](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf) | ECCV | 2020 | WRN-28-10 | Transductive | 70.74 ± 0.85 | 84.34 ± 0.53  | [\[PyTorch\]](https://github.com/ElementAI/embedding-propagation) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf)
[EPNet](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf) | ECCV | 2020 | WRN-28-10 | Semi-supervised | 79.22 ± 0.92  | 88.05 ± 0.51  | [\[PyTorch\]](https://github.com/ElementAI/embedding-propagation) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf)
[F. Wu et al.](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730239.pdf) | ECCV | 2020 | Capsule Network | Inductive | 66.43 ± 0.26  | 82.13 ± 0.21  | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730239.pdf)
[Su et al.](https://arxiv.org/pdf/1910.03560.pdf) | ECCV | 2020 | ResNet-18 | Inductive | -  | 76.6 ± 0.7  | [\[PyTorch\]](https://github.com/cvl-umass/fsl_ssl) | [\[Source\]](https://arxiv.org/pdf/1910.03560.pdf)
[TIM](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf) | NeurIPS | 2020 | ResNet-18 | Transductive | 73.9  | 85.0  | [\[PyTorch\]](https://github.com/mboudiaf/TIM) | [\[Source\]](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf)
[TIM](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf) | NeurIPS | 2020 | WRN-28-10 | Transductive | 77.8  | 87.4  | [\[PyTorch\]](https://github.com/mboudiaf/TIM) | [\[Source\]](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf)
[IFSL](https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf) | NeurIPS | 2020 | WRN-28-10 | Transductive | 73.51  | 83.21  | [\[PyTorch\]](https://github.com/yue-zhongqi/ifsl) | [\[Source\]](https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf)
[DKT](https://proceedings.neurips.cc/paper/2020/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf) | NeurIPS | 2020 | 4CONV| Inductive | 49.73 ± 0.07 | -  | [\[PyTorch\]](https://github.com/BayesWatch/deep-kernel-transfer) | [\[Source\]](https://proceedings.neurips.cc/paper/2020/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf)
[LR+DC](https://openreview.net/pdf?id=JWOiYxMG92s)     | ICLR   | 2021 | WRN-28-10  | Inductive | 68.57 ± 0.55    | 82.88 ± 0.42     | [\[PyTorch\]](https://github.com/ShuoYang-1998/ICLR2021-Oral_Distribution_Calibration) | [\[Source\]](https://openreview.net/pdf?id=JWOiYxMG92s)
[IEPT](https://openreview.net/pdf?id=xzqLpqRzxLq)     | ICLR   | 2021 | ResNet-12  | Inductive | 67.05 ± 0.44    | 82.90 ± 0.30     | None | [\[Source\]](https://openreview.net/pdf?id=xzqLpqRzxLq)
[IEPT](https://openreview.net/pdf?id=xzqLpqRzxLq)     | ICLR   | 2021 | 4CONV  | Inductive | 56.26 ± 0.45    | 73.91 ± 0.34     | None | [\[Source\]](https://openreview.net/pdf?id=xzqLpqRzxLq)
[MELR](https://openreview.net/pdf?id=D3PcGLdMx0)     | ICLR   | 2021 | ResNet-12  | Inductive | 67.40 ± 0.43    | 83.40 ± 0.28     | None | [\[Source\]](https://openreview.net/pdf?id=D3PcGLdMx0)
[MELR](https://openreview.net/pdf?id=D3PcGLdMx0)     | ICLR   | 2021 | 4CONV  | Inductive | 55.35 ± 0.43    | 72.27 ± 0.35     | None | [\[Source\]](https://openreview.net/pdf?id=D3PcGLdMx0)
[ConstellationNet](https://openreview.net/pdf?id=vujTf_I8Kmc)     | ICLR   | 2021 | ResNet-12   | Inductive | 64.89 ± 0.23    | 79.95 ± 0.37     | None | [\[Source\]](https://openreview.net/pdf?id=vujTf_I8Kmc)
[OVE PG G P+ Cosine (ML)](https://openreview.net/pdf?id=lgNx56yZh8a)     | ICLR   | 2021 | 4CONV  | Inductive | 50.02 ± 0.35    | 64.58 ± 0.31     | None | [\[Source\]](https://openreview.net/pdf?id=lgNx56yZh8a)
[OVE PG G P+ Cosine (PL)](https://openreview.net/pdf?id=lgNx56yZh8a)     | ICLR   | 2021 | 4CONV  | Inductive | 48.00 ± 0.24    | 67.14 ± 0.23     | None | [\[Source\]](https://openreview.net/pdf?id=lgNx56yZh8a)
[BOIL](https://openreview.net/pdf?id=umIdUL8rMH)     | ICLR   | 2021 | 4CONV  | Inductive | 49.61 ± 0.16    | 66.45 ± 0.37     | None | [\[Source\]](https://openreview.net/pdf?id=umIdUL8rMH)
[FRN](https://arxiv.org/pdf/2012.01506.pdf) | CVPR | 2021 | ResNet-12 | Inductive |  66.45 ± 0.19 | 82.83 ± 0.13  | [\[PyTorch\]](https://github.com/Tsingularity/FRN) | [\[Source\]](https://arxiv.org/pdf/2012.01506.pdf)
[PT+MAP](https://arxiv.org/pdf/2006.03806v3.pdf) | arXiv | 2021 | WRN | Inductive |  65.35 ± 0.20 | 83.87 ± 0.13  | [\[PyTorch\]](https://github.com/yhu01/PT-MAP) | [\[Source\]](https://arxiv.org/pdf/2006.03806v3.pdf)
[PT+MAP](https://arxiv.org/pdf/2006.03806v3.pdf) | arXiv | 2021 | WRN | Transductive |  82.92 ± 0.26 | 88.82 ± 0.13  | [\[PyTorch\]](https://github.com/yhu01/PT-MAP) | [\[Source\]](https://arxiv.org/pdf/2006.03806v3.pdf)
[CSEI](https://www.aaai.org/AAAI21Papers/AAAI-540.LiJ.pdf) | AAAI | 2021 | ResNet-12 | Inductive |  68.94 ± 0.28 | 85.07 ± 0.50  | None | [\[Source\]](https://www.aaai.org/AAAI21Papers/AAAI-540.LiJ.pdf)
[PTN](https://arxiv.org/pdf/2012.10844.pdf) | AAAI | 2021 | WRN-28-10  | Semi-supervised |  82.66 ± 0.97 | 88.43 ± 0.67  | None | [\[Source\]](https://arxiv.org/pdf/2012.10844.pdf)
[P-Transfer](https://www.aaai.org/AAAI21Papers/AAAI-1041.ShenZ.pdf) | AAAI | 2021 | ResNet-12  | Inductive |  64.21 ± 0.77 | 80.38 ± 0.59  | None | [\[Source\]](https://www.aaai.org/AAAI21Papers/AAAI-1041.ShenZ.pdf)
[CNL](https://www.aaai.org/AAAI21Papers/AAAI-3486.ZhaoJ.pdf) | AAAI | 2021 | ResNet-12  | Inductive |  67.96 ± 0.98 | 83.36 ± 0.51 | None | [\[Source\]](https://www.aaai.org/AAAI21Papers/AAAI-3486.ZhaoJ.pdf)
[TACO](https://www.aaai.org/AAAI21Papers/AAAI-5922.YeHJ.pdf) | AAAI | 2021 | ResNet-12  | Semi-supervised | 68.23 | 83.42 | None | [\[Source\]](https://www.aaai.org/AAAI21Papers/AAAI-5922.YeHJ.pdf)
[InfoPatch](https://www.aaai.org/AAAI21Papers/AAAI-2249.LiuC.pdf) | AAAI | 2021 | ResNet-12  | Inductive |  67.67±0.45  | 82.44±0.31  | [\[PyTorch\]](https://github.com/corwinliu9669/Learning-a-Few-shot-Embedding-Model-with-Contrastive-Learning) | [\[Source\]](https://www.aaai.org/AAAI21Papers/AAAI-2249.LiuC.pdf)
[DMF](https://arxiv.org/pdf/2103.13582.pdf) | CVPR | 2021 | ResNet-12  | Inductive |  67.76±0.46  | 82.71±0.31  | [\[PyTorch\]](https://github.com/loadder/Dynamic-Meta-filter) | [\[Source\]](https://arxiv.org/pdf/2103.13582.pdf)
[ECKPN](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | 4CONV  | Transductive | 68.89±0.34  | 83.59±0.44  | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf)
[ECKPN](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12  | Transductive | 70.48±0.38  | 85.42±0.46  | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf)
[MCGN](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | [64-96-128-256](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf)  | Inductive | 57.89±0.87   | 73.58±0.87 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf)
[MCGN](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | [64-96-128-256](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf) | Transductive | 67.32±0.43  | 83.03±0.54  | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf)
[ArL](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Rethinking_Class_Relations_Absolute-Relative_Supervised_and_Unsupervised_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12  | Semi-supervised | 65.21±0.58 | 80.41±0.49 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Rethinking_Class_Relations_Absolute-Relative_Supervised_and_Unsupervised_Few-Shot_Learning_CVPR_2021_paper.pdf)
[PSST](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12 | Inductive | 64.05±0.49  | 80.24±0.45 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.pdf)
[PSST](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | WRN-28-10 | Inductive | 64.16±0.44 | 80.64±0.32 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.pdf)
[RAP-LaplacianShot](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12 | Transductive | 74.29±0.20  | 84.51±0.13 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.pdf)
[RAP-LaplacianShot](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.pdf) | CVPR | 2021 | DenseNet-121 | Transductive | 75.58±0.20  | 85.63±0.13 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.pdf)
[LaplacianShot](http://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf) | ICML | 2020 | DenseNet-121 | Transductive | 75.57±0.19   | 84.72±0.13 | [\[PyTorch\]](https://github.com/imtiazziko/LaplacianShot) | [\[Source\]](http://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf)
[LaplacianShot](http://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf) | ICML | 2020 | WRN | Transductive | 74.86±0.19   | 84.13±0.14 | [\[PyTorch\]](https://github.com/imtiazziko/LaplacianShot) | [\[Source\]](http://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf)
[LaplacianShot](http://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf) | ICML | 2020 | ResNet-18 | Transductive | 72.11±0.19   | 82.31±0.14 | [\[PyTorch\]](https://github.com/imtiazziko/LaplacianShot) | [\[Source\]](http://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf)
[Rizve et al.](https://openaccess.thecvf.com/content/CVPR2021/papers/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12 | Inductive | 67.28±0.80  |  84.78±0.52 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.pdf)
[Zhang et al.](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Prototype_Completion_With_Primitive_Knowledge_for_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12 | Transductive | 73.13±0.85 | 82.06±0.54 | [\[PyTorch\]](https://github.com/zhangbq-research/Prototype_Completion_for_FSL) | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Prototype_Completion_With_Primitive_Knowledge_for_Few-Shot_Learning_CVPR_2021_paper.pdf)
[IEPT+ZN](https://openaccess.thecvf.com/content/ICCV2021/papers/Fei_Z-Score_Normalization_Hubness_and_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 67.35±0.43 | 83.04±0.29 | None | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Fei_Z-Score_Normalization_Hubness_and_Few-Shot_Learning_ICCV_2021_paper.pdf)
[PLCM](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Pseudo-Loss_Confidence_Metric_for_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Semi-supervised | 72.06±1.08 | 83.71±0.63 | [\[PyTorch\]](https://github.com/wyimhhjjs/PLCM) | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Pseudo-Loss_Confidence_Metric_for_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[PAL](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Partner-Assisted_Learning_for_Few-Shot_Image_Classification_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 69.37±0.64 | 84.40±0.44 | None | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Partner-Assisted_Learning_for_Few-Shot_Image_Classification_ICCV_2021_paper.pdf)
[ALFA+MeTAL](https://openaccess.thecvf.com/content/ICCV2021/papers/Baik_Meta-Learning_With_Task-Adaptive_Loss_Function_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Transductive | 66.61±0.28 | 84.40±0.44 |  [\[PyTorch\]](https://github.com/baiksung/MeTAL)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Baik_Meta-Learning_With_Task-Adaptive_Loss_Function_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[Curvature Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 (64-160-320-640) | Inductive | 67.02±0.20 | 82.32±0.14 |  [\[PyTorch\]](https://github.com/ZhiGaomcislab/CurvatureGeneration_FSL)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[Curvature Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 (64-160-320-640) | Transductive | 71.79±0.23 | 83.00±0.17 |  [\[PyTorch\]](https://github.com/ZhiGaomcislab/CurvatureGeneration_FSL)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[TPMN](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Task-Aware_Part_Mining_Network_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 67.64±0.63 | 83.44±0.43 | None | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Task-Aware_Part_Mining_Network_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[Meta-Baseline](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Meta-Baseline_Exploring_Simple_Meta-Learning_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 63.17±0.23 | 79.26±0.17 |  [\[PyTorch\]](https://github.com/yinboc/few-shot-meta-baseline)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Meta-Baseline_Exploring_Simple_Meta-Learning_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[RENet](https://openaccess.thecvf.com/content/ICCV2021/papers/Kang_Relational_Embedding_for_Few-Shot_Classification_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 67.60±0.44 | 82.58±0.30 |  [\[PyTorch\]](https://github.com/dahyun-kang/renet)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Kang_Relational_Embedding_for_Few-Shot_Classification_ICCV_2021_paper.pdf)
[BML](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Binocular_Mutual_Learning_for_Improving_Few-Shot_Classification_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 67.04±0.63 | 83.63±0.29 |  [\[PyTorch\]](https://github.com/ZZQzzq/BML)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Binocular_Mutual_Learning_for_Improving_Few-Shot_Classification_ICCV_2021_paper.pdf)
[MetaQDA](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Shallow_Bayesian_Meta_Learning_for_Real-World_Few-Shot_Recognition_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Inductive | 67.83±0.64 | 84.28±0.69 |  [\[PyTorch\]](https://github.com/Open-Debin/Bayesian_MQDA)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Shallow_Bayesian_Meta_Learning_for_Real-World_Few-Shot_Recognition_ICCV_2021_paper.pdf)
[Meta Navigator](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Meta_Navigator_Search_for_a_Good_Adaptation_Policy_for_Few-Shot_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 67.14±0.80 | 83.82±0.51  |  None  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Meta_Navigator_Search_for_a_Good_Adaptation_Policy_for_Few-Shot_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Transductive |  69.79±0.99 | 79.82±0.55  |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Transductive |  83.05±0.79 | 88.82±0.42  |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Semi-supervised |  70.99±0.91 | 81.06±0.49  |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Semi-supervised  |  83.58±0.79 | 89.68±0.37 |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[AIM](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Few-Shot_and_Continual_Learning_With_Attentive_Independent_Mechanisms_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Transductive |  71.22±0.57 | 82.25±0.34 |  [\[PyTorch\]](https://github.com/huang50213/AIM-Fewshot-Continual)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Few-Shot_and_Continual_Learning_With_Attentive_Independent_Mechanisms_ICCV_2021_paper.pdf)
[MixtFSL](https://openaccess.thecvf.com/content/ICCV2021/papers/Afrasiyabi_Mixture-Based_Feature_Space_Learning_for_Few-Shot_Image_Classification_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 63.98±0.79 | 82.04±0.49 |  [\[PyTorch\]](https://github.com/ArmanAfrasiyabi/MixtFSL-fs)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Afrasiyabi_Mixture-Based_Feature_Space_Learning_for_Few-Shot_Image_Classification_ICCV_2021_paper.pdf)
[Oblique Manifold](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_Transductive_Few-Shot_Classification_on_the_Oblique_Manifold_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Transductive | 80.64±0.34 | 89.39±0.39 | None | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_Transductive_Few-Shot_Classification_on_the_Oblique_Manifold_ICCV_2021_paper.pdf)
[COSOC](https://papers.nips.cc/paper/2021/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf) | NeurIPS | 2021 | ResNet-12 | Inductive | 69.28±0.49 | 85.16±0.42 | [\[PyTorch\]](https://github.com/Frankluox/LightningFSL) | [\[Source\]](https://papers.nips.cc/paper/2021/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf)
[Wang et al.](https://papers.nips.cc/paper/2021/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf) | NeurIPS | 2021 | 4CONV | Inductive | 56.32±0.28 | 72.64±0.26 | None | [\[Source\]](https://papers.nips.cc/paper/2021/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf)
[POODLE](https://papers.nips.cc/paper/2021/file/c91591a8d461c2869b9f535ded3e213e-Paper.pdf) | NeurIPS | 2021 | ResNet-12 | Inductive | 67.80 | 83.72 | None | [\[Source\]](https://papers.nips.cc/paper/2021/file/c91591a8d461c2869b9f535ded3e213e-Paper.pdf)
[POODLE](https://papers.nips.cc/paper/2021/file/c91591a8d461c2869b9f535ded3e213e-Paper.pdf) | NeurIPS | 2021 | ResNet-12 | Transductive | 77.56 | 85.81 | None | [\[Source\]](https://papers.nips.cc/paper/2021/file/c91591a8d461c2869b9f535ded3e213e-Paper.pdf)
[NCA nearest centroid](https://papers.nips.cc/paper/2021/file/cdfa4c42f465a5a66871587c69fcfa34-Paper.pdf) | NeurIPS | 2021 | ResNet-12 | Inductive | 62.55±0.12 | 78.27±0.09 | [\[PyTorch\]](https://github.com/fiveai/on-episodes-fsl) | [\[Source\]](https://papers.nips.cc/paper/2021/file/cdfa4c42f465a5a66871587c69fcfa34-Paper.pdf)
[SSR](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf) | NeurIPS | 2021 | ResNet-12 | Transductive | 68.1±0.6 | 76.9±0.4 | [\[PyTorch\]](https://github.com/XiSHEN0220/SSR) | [\[Source\]](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf)
[SSR](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf) | NeurIPS | 2021 | WRN-28-10 | Transductive | 72.4±0.6 | 80.2±0.4 | [\[PyTorch\]](https://github.com/XiSHEN0220/SSR) | [\[Source\]](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf)
[Simple CNAPS](https://arxiv.org/pdf/1912.03432.pdf) | CVPR | 2020 | ResNet18 (pre-trained on ImageNet) | Inductive | 82.16  | 89.80 | [\[PyTorch\]](https://github.com/plai-group/simple-cnaps) | [\[Source\]](https://arxiv.org/pdf/1912.03432.pdf)
